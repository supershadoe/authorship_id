# Authorship Identification Using Stylometric Features

This project explores authorship identification techniques for electronic
texts, drawing inspiration from methods and evaluations presented in recent
research. The goal is to classify authors based on their writing style
(stylometry) using features derived from Part-of-Speech (POS) tags.

### methodology

This project combines several techniques discussed in the literature:

1. n-grams of Part-of-Speech (POS) tags are used as features to capture
   syntactic patterns, a key aspect of stylometry aimed at being less
   topic-dependent [^1]
2. Extracted POS tag sequences are converted into numerical vectors using
   TF-IDF primarily [^2] but also compared with CountVectorizer due to its
   use in Khonji et al. and the values are normalized.
3. To handle the high dimensionality of n-gram features and address memory
   constraints, especially with tree-based models on the Enron dataset,
   Latent Semantic Analysis (LSA) is performed using TruncatedSVD followed
   by Linear Discriminant Analysis to reduce the dimensions further [^3]

> [!NOTE]
> Although the cited paper uses PCA followed by LDA, it is more memory
> intensive to use it on vectors generated by TF-IDF which gives a huge
> sparse matrix. TruncatedSVD is a better tool as it operates on sparse
> matrices and it is made for such purposes.

4. After such preprocessing, the resulting vectors are fed to a range of
   classifier models such as Random Forest, Decision Tree, Support Vector
   Machine, Logistic Regression, K-nearest Neighbors, and Multi-layer
   Perceptron.

### dataset

Two datasets are used for evaluation:

1.  Enron Email Dataset:
      - Download the 2015 version from [CMU](https://www.cs.cmu.edu/~enron/).
      - Extract it to the `enron_dataset` directory at the project root.
2.  PAN12 Author Identification: Attribution Challenge (Task C):
      - Download from [Zenodo](https://doi.org/10.5281/zenodo.3713273).
      - From the 2012 corpus, extract only the `12Ctrain*.txt` files into
        `pan_dataset/train` directory at the project root.
      - Extract the `12Ctest*.txt` files into `pan_dataset/test` directory.

### installation

`pyproject.toml` file lists all the dependencies for this project.

Use `pip install -e .[dev]` and
`pip install -e .[torch-gpu] --index-url https://download.pytorch.org/whl/cu124`
if installing using pip manually in a virtualenv/conda env.

Use `uv sync` if using [uv](https://docs.astral.sh/uv/) for simpler
management of deps.

### usage

- `preprocess_enron.ipynb`: Contains all code for processing the full Enron
  dataset.
- `enron_dataset_results.ipynb`: Trains and tests the models on the processed
  Enron dataset.
- `pan_dataset_results.ipynb`: Contains both preprocessing and training code
  for PAN12 dataset.


[^1]: M. Khonji, Y. Iraqi and L. Mekouar, "Authorship Identification of Electronic Texts," in IEEE Access, vol. 9, pp. 101124-101146, 2021, doi: 10.1109/ACCESS.2021.3098192.
[^2]: K. A. Apoorva and S. Sangeetha, "Deep neural network and model-based clustering technique for forensic electronic mail author attribution," SN Applied Sciences, vol. 3, no. 3, p. 348, Mar. 2021, doi: 10.1007/s42452-020-04127-6.
[^3]: C. Zhang, X. Wu, Z. Niu, and W. Ding, "Authorship identification from unstructured texts," Knowledge-Based Systems, vol. 66, pp. 99â€“111, Aug. 2014, doi: 10.1016/j.knosys.2014.04.025.